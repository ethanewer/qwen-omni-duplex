{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f7a5188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "from data_util import iter_audio_samples\n",
    "from model import (\n",
    "    QwenWithCausalAudioEncoderConfig,\n",
    "    QwenWithCausalAudioEncoderForConditionalGeneration,\n",
    "    process_qwen_with_causal_audio_encoder_inputs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f197f370",
   "metadata": {},
   "outputs": [],
   "source": [
    "tts_data_path = Path(\"/\") / \"mnt/efs/fs1/wbl/webdataset/webdataset/train/tts_en\"\n",
    "audio_data_file = max(p for p in tts_data_path.iterdir() if p.is_file() and p.suffix in [\".tar\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d395eb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section', 'interleaved', 'mrope_interleaved'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e3b6eb3f164554be1ff3dd36ce880e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers.models.qwen3_omni_moe import Qwen3OmniMoeProcessor\n",
    "\n",
    "model = QwenWithCausalAudioEncoderForConditionalGeneration.from_pretrained(\n",
    "    \"eewer/Qwen3WithMimi-30B-A3B\",\n",
    "    config=QwenWithCausalAudioEncoderConfig.from_pretrained(\"eewer/Qwen3WithMimi-30B-A3B\"),\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:4\",\n",
    ")\n",
    "processor = Qwen3OmniMoeProcessor.from_pretrained(\"Qwen/Qwen3-Omni-30B-A3B-Instruct\")  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab75eaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.models.qwen2_5_omni import Qwen2_5OmniProcessor\n",
    "\n",
    "# model = QwenWithCausalAudioEncoderForConditionalGeneration.from_pretrained(\n",
    "#     \"eewer/Qwen2.5WithMimi-3B\",\n",
    "#     config=QwenWithCausalAudioEncoderConfig.from_pretrained(\"eewer/Qwen2.5WithMimi-3B\"),\n",
    "#     dtype=torch.bfloat16,\n",
    "#     device_map=\"cuda:2\",\n",
    "# )\n",
    "# processor = Qwen2_5OmniProcessor.from_pretrained(\"Qwen/Qwen2.5-Omni-3B\")  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12febcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_iter = iter_audio_samples(audio_data_file)\n",
    "samples = [next(sample_iter) for _ in range(2)]\n",
    "conversations = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\",\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Transcribe the English audio into text without any punctuation marks.\"},\n",
    "                {\"type\": \"audio\", \"audio\": None},\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "] * len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f241c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = process_qwen_with_causal_audio_encoder_inputs(\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    conversation=conversations,\n",
    "    audio=[sample.audio for sample in samples],\n",
    "    audio_sample_rate=[sample.audio_sample_rate for sample in samples],\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "sequences = model.generate_greedy(**inputs)\n",
    "outputs = processor.tokenizer.batch_decode(sequences, skip_special_tokens=True)  # type: ignore\n",
    "\n",
    "for sample, output in zip(samples, outputs):\n",
    "    print(f\"<true_text>\\n{sample.text}\\n</true_text>\\n<model_output>\\n{output}\\n</model_output>\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6d0f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omni-to-duplex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
