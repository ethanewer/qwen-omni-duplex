{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f7a5188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from transformers.models.qwen2_5_omni import Qwen2_5OmniProcessor\n",
    "\n",
    "from data_util import iter_audio_samples\n",
    "from model import (\n",
    "    QwenWithCausalAudioEncoderConfig,\n",
    "    QwenWithCausalAudioEncoderForConditionalGeneration,\n",
    "    process_qwen_with_mimi_inputs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f197f370",
   "metadata": {},
   "outputs": [],
   "source": [
    "tts_data_path = Path(\"/\") / \"mnt/efs/fs1/wbl/webdataset/webdataset/train/tts_en\"\n",
    "audio_data_file = max(p for p in tts_data_path.iterdir() if p.is_file() and p.suffix in [\".tar\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab75eaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e58c4f7a7a4e1794eaaa654c8aee0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    }
   ],
   "source": [
    "model = QwenWithCausalAudioEncoderForConditionalGeneration.from_pretrained(\n",
    "    \"eewer/Qwen2.5WithMimi-3B\",\n",
    "    config=QwenWithCausalAudioEncoderConfig.from_pretrained(\"eewer/Qwen2.5WithMimi-3B\"),\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:2\",\n",
    ")\n",
    "processor = Qwen2_5OmniProcessor.from_pretrained(\"Qwen/Qwen2.5-Omni-3B\")  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c12febcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_iter = iter_audio_samples(audio_data_file)\n",
    "samples = [next(sample_iter) for _ in range(2)]\n",
    "conversations = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\",\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Transcribe the English audio into text without any punctuation marks.\"},\n",
    "                {\"type\": \"audio\", \"audio\": None},\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "] * len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f241c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<true_text>\n",
      "I can't say anything about any figure.\n",
      "</true_text>\n",
      "<model_output>\n",
      "system\n",
      "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\n",
      "user\n",
      "Transcribe the English audio into text without any punctuation marks.\n",
      "assistant\n",
      "I can't say anything about any figure.\n",
      "</model_output>\n",
      "\n",
      "<true_text>\n",
      "A counter reaction was.\n",
      "</true_text>\n",
      "<model_output>\n",
      "system\n",
      "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\n",
      "user\n",
      "Transcribe the English audio into text without any punctuation marks.\n",
      "assistant\n",
      "A counter - reaction was.\n",
      "Human\n",
      "</model_output>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = process_qwen_with_mimi_inputs(\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    conversation=conversations,\n",
    "    audio=[sample.audio for sample in samples],\n",
    "    audio_sample_rate=[sample.audio_sample_rate for sample in samples],\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "sequences = model.generate_greedy(**inputs)\n",
    "outputs = processor.tokenizer.batch_decode(sequences, skip_special_tokens=True)  # type: ignore\n",
    "\n",
    "for sample, output in zip(samples, outputs):\n",
    "    print(f\"<true_text>\\n{sample.text}\\n</true_text>\\n<model_output>\\n{output}\\n</model_output>\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a66193",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omni-to-duplex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
