{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f7a5188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "from data_util import iter_audio_samples\n",
    "from model import (\n",
    "    QwenWithCausalAudioEncoderConfig,\n",
    "    QwenWithCausalAudioEncoderForConditionalGeneration,\n",
    "    process_qwen_with_causal_audio_encoder_inputs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f197f370",
   "metadata": {},
   "outputs": [],
   "source": [
    "tts_data_path = Path(\"/\") / \"mnt/efs/fs1/wbl/webdataset/webdataset/train/tts_en\"\n",
    "audio_data_file = max(p for p in tts_data_path.iterdir() if p.is_file() and p.suffix in [\".tar\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d395eb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_interleaved', 'interleaved', 'mrope_section'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7888cd827f461b955ce71fcfef3a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers.models.qwen3_omni_moe import Qwen3OmniMoeProcessor\n",
    "\n",
    "model = QwenWithCausalAudioEncoderForConditionalGeneration.from_pretrained(\n",
    "    \"eewer/Qwen3WithMimi-30B-A3B\",\n",
    "    config=QwenWithCausalAudioEncoderConfig.from_pretrained(\"eewer/Qwen3WithMimi-30B-A3B\"),\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:4\",\n",
    ")\n",
    "processor = Qwen3OmniMoeProcessor.from_pretrained(\"Qwen/Qwen3-Omni-30B-A3B-Instruct\")  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab75eaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.models.qwen2_5_omni import Qwen2_5OmniProcessor\n",
    "\n",
    "# model = QwenWithCausalAudioEncoderForConditionalGeneration.from_pretrained(\n",
    "#     \"eewer/Qwen2.5WithMimi-3B\",\n",
    "#     config=QwenWithCausalAudioEncoderConfig.from_pretrained(\"eewer/Qwen2.5WithMimi-3B\"),\n",
    "#     dtype=torch.bfloat16,\n",
    "#     device_map=\"cuda:2\",\n",
    "# )\n",
    "# processor = Qwen2_5OmniProcessor.from_pretrained(\"Qwen/Qwen2.5-Omni-3B\")  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c12febcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_iter = iter_audio_samples(audio_data_file)\n",
    "samples = [next(sample_iter) for _ in range(2)]\n",
    "conversations = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\",\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Transcribe the English audio into text without any punctuation marks.\"},\n",
    "                {\"type\": \"audio\", \"audio\": None},\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "] * len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f241c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<true_text>\n",
      "I can't say anything about any figure.\n",
      "</true_text>\n",
      "<model_output>\n",
      "system\n",
      "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\n",
      "user\n",
      "Transcribe the English audio into text without any punctuation marks.\n",
      "assistant\n",
      "I can't say anything about any figure.\n",
      "</model_output>\n",
      "\n",
      "<true_text>\n",
      "A counter reaction was.\n",
      "</true_text>\n",
      "<model_output>\n",
      "system\n",
      "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\n",
      "user\n",
      "Transcribe the English audio into text without any punctuation marks.\n",
      "assistant\n",
      "A counter reaction was\n",
      "user\n",
      "\n",
      "</model_output>\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "inputs = process_qwen_with_causal_audio_encoder_inputs(\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    conversation=conversations,\n",
    "    audio=[sample.audio for sample in samples],\n",
    "    audio_sample_rate=[sample.audio_sample_rate for sample in samples],\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "sequences = model.generate_greedy(**inputs)\n",
    "outputs = processor.tokenizer.batch_decode(sequences, skip_special_tokens=True)  # type: ignore\n",
    "\n",
    "for sample, output in zip(samples, outputs):\n",
    "    print(f\"<true_text>\\n{sample.text}\\n</true_text>\\n<model_output>\\n{output}\\n</model_output>\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e03d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omni-to-duplex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
